{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***\n",
    "***\n",
    "# 对大数据进行预处理\n",
    "\n",
    "以占领华尔街推特数据为例\n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "王成军\n",
    "\n",
    "wangchengjun@nju.edu.cn\n",
    "\n",
    "计算传播网 http://computational-communication.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 字节（Byte /bait/）\n",
    "\n",
    "计算机信息技术用于计量存储容量的一种计量单位，通常情况下一字节等于有八位， [1]  也表示一些计算机编程语言中的数据类型和语言字符。\n",
    "- 1B（byte，字节）= 8 bit；\n",
    "- 1KB=1000B；1MB=1000KB=1000×1000B。其中1000=10^3。\n",
    "- 1KB（kilobyte，千字节）=1000B= 10^3 B；\n",
    "- 1MB（Megabyte，兆字节，百万字节，简称“兆”）=1000KB= 10^6 B；\n",
    "- 1GB（Gigabyte，吉字节，十亿字节，又称“千兆”）=1000MB= 10^9 B；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 按照Chunk读取数据并进行处理\n",
    "\n",
    "Lazy Method for Reading Big File in Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2752\n"
     ]
    }
   ],
   "source": [
    "bigfile = open('/Users/chengjun/百度云同步盘/Writing/OWS/ows-raw.txt', 'r')\n",
    "chunkSize = 1000000\n",
    "chunk = bigfile.readlines(chunkSize)\n",
    "print(len(chunk))\n",
    "with open(\"/Users/chengjun/GitHub/cjc/data/ows_tweets_sample.txt\", 'w') as f:\n",
    "    for i in chunk:\n",
    "        f.write(i)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T07:31:51.644484Z",
     "start_time": "2019-06-08T07:30:56.170308Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 262665\n",
      "1 525130\n",
      "2 787344\n",
      "3 1049351\n",
      "4 1312571\n",
      "5 1574666\n",
      "6 1835628\n",
      "7 2097136\n",
      "8 2358494\n",
      "9 2619723\n",
      "10 2880857\n",
      "11 3140945\n",
      "12 3404775\n",
      "13 3665565\n",
      "14 3927996\n",
      "15 4189419\n",
      "16 4449078\n",
      "17 4709001\n",
      "18 4969877\n",
      "19 5230937\n",
      "20 5492578\n",
      "21 5756613\n",
      "22 6022478\n",
      "23 6286119\n",
      "24 6549476\n",
      "25 6602141\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/519633/lazy-method-for-reading-big-file-in-python?lq=1\n",
    "import csv\n",
    "bigfile = open('/Users/datalab/bigdata/cjc/ows-raw.txt', 'r')\n",
    "\n",
    "chunkSize = 10**8\n",
    "chunk = bigfile.readlines(chunkSize)\n",
    "num, num_lines = 0, 0\n",
    "while chunk:\n",
    "    lines = csv.reader((line.replace('\\x00','') for line in chunk), \n",
    "                       delimiter=',', quotechar='\"')\n",
    "    #do sth.\n",
    "    num_lines += len(list(lines))\n",
    "    print(num, num_lines)\n",
    "    num += 1\n",
    "    chunk = bigfile.readlines(chunkSize) # read another chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 用Pandas的get_chunk功能来处理亿级数据\n",
    "\n",
    "> 只有在超过5TB数据量的规模下，Hadoop才是一个合理的技术选择。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "f = open('../bigdata/OWS/ows-raw.txt',encoding='utf-8')\n",
    "reader = pd.read_table(f,  sep=',',  iterator=True, error_bad_lines=False) #跳过报错行\n",
    "loop = True\n",
    "chunkSize = 100000\n",
    "data = []\n",
    "\n",
    "while loop:\n",
    "    try:\n",
    "        chunk = reader.get_chunk(chunkSize)\n",
    "        dat = data_cleaning_funtion(chunk) # do sth.\n",
    "        data.append(dat) \n",
    "    except StopIteration:\n",
    "        loop = False\n",
    "        print(\"Iteration is stopped.\")\n",
    "\n",
    "df = pd.concat(data, ignore_index=True)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "1260px",
    "left": "1835px",
    "top": "224px",
    "width": "512px"
   },
   "toc_section_display": false,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
